'''
XXX:

We use fixed random seeds that was generated by:

    hex(randint(0, 2**32))

This ensures a stable pseudo-random distribution.

Author:     Pontus Stenetorp    <pontus stenetorp se>
Version:    2011-03-02
'''

from collections import defaultdict, namedtuple
from itertools import chain, tee
from os import walk
from os.path import dirname, splitext
from os.path import join as join_path
from re import compile as _compile
from random import sample
from sys import path

from codecs import EncodedFile

from aligner import Aligner, MisalignedError

path.append(join_path(dirname(__file__), '../'))

from resources import Annotation, Document, Sentence

### Constants
TXT_EXT = '.txt'
SS_EXT = '.ss'
A1_EXT = '.a1'
A2_EXT = '.a2'
RES_EXTS = set((TXT_EXT, SS_EXT, A1_EXT, A2_EXT))
# We use this mapping for sorting
PRIO_BY_RES_EXT = {
        TXT_EXT: 0,
        SS_EXT: 10,
        A1_EXT: 20,
        A2_EXT: 30,
        }
TB_SO_REGEX = _compile((r'^T[0-9]+\t(?P<type>[^ ]+) (?P<start>\d+) '
    r'(?P<end>\d+)(?:\t(?P<text>.*?))?$'))
# TODO: data dir could be referred elsewhere
DATA_DIR = join_path(dirname(__file__), '../data/corpora')
BIONLP_2011_DIR = join_path(DATA_DIR, 'bionlp_2011_st')
BIONLP_2009_DIR = join_path(DATA_DIR, 'bionlp_2009_st')
GREC_DIR = join_path(DATA_DIR, 'grec')
CALBC_DIR = join_path(DATA_DIR,
        'calbc_ii_st_format_500_sample')
NLPBA_DIR = join_path(DATA_DIR,
        'nlpba_slightly_wrong')
NLPBA_DOWN_DIR = join_path(DATA_DIR,
        'nlpba_slightly_wrong_downsized')
#        'CALBC.CII.75k.cos98.3.EBI.CL.20101008_st_format')
# Epi and PTM data
BIONLP_2011_EPI_TRAIN_DIR = join_path(BIONLP_2011_DIR,
        'BioNLP-ST_2011_Epi_and_PTM_training_data')
BIONLP_2011_EPI_DEV_DIR = join_path(BIONLP_2011_DIR,
        'BioNLP-ST_2011_Epi_and_PTM_development_data')
# Genia data
BIONLP_2011_GENIA_TRAIN_DIR = join_path(BIONLP_2011_DIR,
        'BioNLP-ST_2011_genia_train_data_rev1')
BIONLP_2011_GENIA_DEV_DIR = join_path(BIONLP_2011_DIR,
        'BioNLP-ST_2011_genia_devel_data_rev1')
# Infectious Diseases (ID) data
BIONLP_2011_ID_TRAIN_DIR = join_path(BIONLP_2011_DIR,
        'BioNLP-ST_2011_Infectious_Diseases_training_data')
BIONLP_2011_ID_DEV_DIR = join_path(BIONLP_2011_DIR,
        'BioNLP-ST_2011_Infectious_Diseases_development_data')
# BioNLP'09 Shared Task data
BIONLP_2009_DEV_DIR = join_path(BIONLP_2009_DIR, 'dev')
BIONLP_2009_TRAIN_DIR = join_path(BIONLP_2009_DIR, 'train')
###


Offset = namedtuple('Offset', ('start', 'end'))


def _extension(path):
    return splitext(path)[1]

def _res_ext_cmp(path, other):
    return cmp(*(PRIO_BY_RES_EXT[_extension(p)] for p in (path, other)))

# Get aligned: id, txt, ss, a1, a2 function
# XXX: This relies heavily on the filenaming conventions for ST'11
def _get_aligned_resources(dir):
    resources_by_id = defaultdict(list)
    for root, _, files in walk(dir):
        # Get all files with resource extensions
        for res_path, res_name in ((join_path(root, filename), filename)
                for filename in files if _extension(filename) in RES_EXTS):
            resources_by_id[splitext(res_name)[0]].append(res_path)
    
    # Sanity checking
    for id, resources in resources_by_id.iteritems():
        exts = set((_extension(res_path) for res_path in resources))
        # Check for extension collisions
        assert len(resources) == len(exts), ('multiple resources '
                    'with the same extension for id: {}, resources: "{}"'
                    ).format(id, ', '.join(resources))

        # Check that all suffixes are present (apart from .a2)
        for ext in (e for e in RES_EXTS if e != A2_EXT):
            assert ext in exts, ('no resource with extension: {} for id: {}'
                    ).format(ext, id)

        # Yield the resources in a determined order
        resources.sort(cmp=_res_ext_cmp)
        ret = [id]
        for res in resources:
            ret.append(res)
        #XXX: HACK!
        if len(ret) == len(RES_EXTS):
            ret.append(None)
            #print ret
        yield ret

NO_STR = True
def _str(s):
    if NO_STR:
        return unicode(s)
    else:
        return str(s)

# XXX: Rename this one!
def _get_sentences_and_offsets(txt_handle, ss_handle):
    s_starts_and_sentences = []
    txt_handle_reads = 0
    for s_text in (l.rstrip('\n') for l in ss_handle):
        # XXX: We allow multiple spaces to be aligned due to issues with the SS
        aligner = Aligner(unicode(s_text, encoding='utf-8'), ignore_mult=set((' ', )))

        t_char = None
        started_at = txt_handle.tell()
        started_at_read = txt_handle_reads
        while True:
            t_char = unicode(txt_handle.read(1), encoding='utf-8')
            txt_handle_reads += 1
            if not t_char:
                assert False, ('could not align all sentences for: '
                        '"{}" and "{}" stopped at the sentence: "{}" '
                        'aligner in state: {}'
                        ).format(txt_handle.name, ss_handle.name,
                                s_text, aligner.__repr__())
            try:
                if aligner.align(t_char):
                    source_text = _str(aligner)

                    # We are aligned!
                    s_starts_and_sentences.append((
                            #txt_handle.tell() - len(source_text),
                            #started_at,
                            started_at_read,
                            Sentence(source_text, [])))
                    #last_end += aligner.char_cnt
                    break
            except MisalignedError:
                started_at = txt_handle.tell()
                started_at_read = txt_handle_reads
                pass
    
    #s_starts_and_sentences.sort()
    return s_starts_and_sentences

from bisect import bisect_left

# Used to get the sentence containing an index
#XXX: Assumes sorted list!
def _find_containing_idx(idx, s_starts_and_sentences):
    # Note the tuple dummy to emulate (s_start, s_text)
    s_idx = bisect_left(s_starts_and_sentences, (idx, ''))
    #if not s_idx:
    #    raise ValueError
    assert s_idx > 0, s_starts_and_sentences
    ret_idx = s_idx - 1
    assert ret_idx >= 0, ret_idx
    return s_starts_and_sentences[ret_idx]

ENCODE_WRAP = True
def _encode_wrap(f):
    return EncodedFile(f, 'utf-8')

def _get_documents(dir):
    for id, txt_path, ss_path, a1_path, a2_path in _get_aligned_resources(dir):
        #print id
        # First we align the text and the sentences since we need to map the
        # offsets of the stand-off to map to the sentences in the sentence
        # split file
        #with open(txt_path, 'r') as txt_file, open(ss_path, 'r') as ss_file:
        with open(txt_path, 'r') as txt_file:
            if ENCODE_WRAP:
                txt_file = _encode_wrap(txt_file)
            with open(ss_path, 'r') as ss_file:
                if ENCODE_WRAP:
                    ss_file = _encode_wrap(ss_file)
                #sentences, s_offset_by_sentence = (
                s_starts_and_sentences = (
                        _get_sentences_and_offsets(txt_file, ss_file))

        #XXX: HACK!
        if a2_path is None:
            a2_path = '/dev/null'

        #with open(a1_path, 'r') as a1_file, open(a2_path, 'r') as a2_file:
        with open(a1_path, 'r') as a1_file:
            if ENCODE_WRAP:
                a1_file = _encode_wrap(a1_file)
            with open(a2_path, 'r') as a2_file:
                if ENCODE_WRAP:
                    a2_file = _encode_wrap(a2_file)
                for line in (l.rstrip('\n') for l in chain(a1_file, a2_file)):
                    # We ignore everything apart from the text-bound annotations
                    match = TB_SO_REGEX.match(line)
                    if match is not None:
                        g_dict = match.groupdict()
                        ann_start = int(g_dict['start'])
                        ann_end = int(g_dict['end'])

                        # Find the sentence and its index containing the annotation
                        s_idx, sentence = _find_containing_idx(ann_start,
                                s_starts_and_sentences)
                        
                        # XXX: There are cases where an annotation is cut-off
                        #       by a sentence break. If this is the case, merge
                        #       the sentences.
                        if ann_end > s_idx + len(sentence.text):
                            next_s_idx, next_sentence = _find_containing_idx(
                                    ann_end, s_starts_and_sentences)
                            # Merge the next sentence into this one
                            # XXX: Just assumes a space! May be wrong!
                            sentence = Sentence(sentence.text + ' ' + next_sentence.text,
                                    sentence.annotations + next_sentence.annotations)
                            # Remove the old one
                            s_starts_and_sentences.remove((next_s_idx, next_sentence))

                        # Create an annotation object but adjust the indices to
                        # be relative to the sentence and not to the file
                        new_ann_start = ann_start - s_idx
                        assert 0 <= new_ann_start < len(sentence.text), '0 <= {} < {} ({}, {}) {} "{}" {}'.format(
                                new_ann_start, len(sentence.text), s_idx, g_dict['start'], id, g_dict['text'], s_idx)
                        new_ann_end = ann_end - s_idx
                        assert 0 < new_ann_end <= len(sentence.text), '0 < {} <= {} ({}, {}) {} {}'.format(
                                new_ann_end, len(sentence.text), s_idx, g_dict['end'], id, g_dict['text'])
                        assert new_ann_start < new_ann_end
                        annotation = Annotation(
                                ann_start - s_idx, ann_end - s_idx, g_dict['type'])

                        # If we have a text span in the stand-off we sanity check
                        # it against what is in the sentence
                        #XXX: Run this again!
                        if g_dict['text'] is not None:
                            g_dict['text'] = unicode(g_dict['text'].strip('\r\n'), encoding='utf-8') #XXX: Regex is not perfect
                            # it leaves spaces around
                            target_ann_text = sentence.annotation_text(annotation)
                            assert target_ann_text == g_dict['text'], (
                                    'text span mismatch in {} '
                                    'target: "{}" != source: "{}" {} "{}" {} {} {}'
                                    ).format(id, target_ann_text, g_dict['text'],
                                            annotation, sentence.text, g_dict,
                                            type(target_ann_text), type(g_dict['text']))

                        sentence.add_annotation(annotation)
                    #else:
                    #    assert False, line.replace(' ', '\s').replace('\t', '\\t')

        yield Document(id, [],
                [sentence for _, sentence in s_starts_and_sentences],
                txt_path)

from random import seed as random_seed

# TODO: Can be generalised to more of a tee
def _evenly_split_it(it, seed=None):
    # Determine the length of the sequence
    cnt_it, it = tee(it, 2)
    cnt = max(i for i, _ in enumerate(cnt_it, start=1))
    
    if seed is not None:
        random_seed(seed)

    # Create a set with the elements to use for the first sequence
    one_indices = set(sample(xrange(cnt), (cnt + 1) / 2))

    # Create and filter two new iterators to give different elements
    nfilt_one, nfilt_two = tee(it, 2)
    return ((e for i, e in enumerate(nfilt_one) if i in one_indices),
            (e for i, e in enumerate(nfilt_two) if i not in one_indices))

'''
_a, _b = _evenly_split_it(xrange(10))
assert max((i for i, _ in enumerate(_a))) == max((i for i, _ in enumerate(_b)))
__a = set([e for e in _a])
__b = set([e for e in _b])
print __a, __b
assert __a & __b == set()
assert False
'''

# hex(randint(0, 2**32))

def get_epi_set():
    # Split the development set in two
    dev_it, test_it = _evenly_split_it(
            _get_documents(BIONLP_2011_EPI_DEV_DIR), seed=0x22612817)
    return (_get_documents(BIONLP_2011_EPI_TRAIN_DIR), dev_it, test_it)

def get_genia_set():
    # Split the development set in two
    dev_it, test_it = _evenly_split_it(
            _get_documents(BIONLP_2011_GENIA_DEV_DIR), seed=0xd81979e3)
    return (_get_documents(BIONLP_2011_GENIA_TRAIN_DIR), dev_it, test_it)

def get_id_set():
    # Split the development set in two
    dev_it, test_it = _evenly_split_it(
            _get_documents(BIONLP_2011_ID_DEV_DIR), seed=0xb2f32d56)

    return (_get_documents(BIONLP_2011_ID_TRAIN_DIR), dev_it, test_it)

# XXX: Does not belong here!!!
# XXX: Hack, just works, (tm)
def get_calbc_ii_set():
    random_seed('0xc78e13c3')
    #XXX: We down-sample to 250, 125, 125
    paths = sample([d.path for d in _get_documents(CALBC_DIR)], 500)
    train_paths = set(sample(paths, len(paths) / 2))
    paths = [p for p in paths if p not in train_paths]
    dev_paths = set(sample(paths, len(paths) / 2))
    test_paths = set([p for p in paths if p not in dev_paths])
    del paths

    #print 'train', train_paths
    #print 'dev', dev_paths
    #print 'test', test_paths

    return (
            (d for d in _get_documents(CALBC_DIR)
                if d.path in train_paths),
            (d for d in _get_documents(CALBC_DIR)
                if d.path in dev_paths),
            (d for d in _get_documents(CALBC_DIR)
                if d.path in test_paths),
            )


def get_bionlp_2009_set():
    # Split the development set in two
    dev_it, test_it = _evenly_split_it(
            _get_documents(BIONLP_2009_DEV_DIR), seed=0xb2f32d56)

    return (_get_documents(BIONLP_2009_TRAIN_DIR), dev_it, test_it)

### GREC Constants
# According to GREC annotation guidlines
GREC_SUPER_CONCEPTS = set((
    'EXPERIMENTAL',
    'LIVING_SYSTEM',
    'NUCLEIC_ACIDS',
    'PROCESSES',
    'PROTEINS',
    ))

GREC_ONTOLOGY = {
        # PROTEINS Sub-ontology
        'PROTEIN_STRUCTURE': 'PROTEINS',
        'Polypeptide': 'PROTEIN_STRUCTURE',
        'Peptide': 'PROTEIN_STRUCTURE',
        'Protein_Complex': 'PROTEIN_STRUCTURE',
        'Protein': 'PROTEIN_STRUCTURE',
        'Amino_Acids': 'PROTEIN_STRUCTURE',
        'Conformation': 'Polypeptide',
        'Sub_Unit': 'Polypeptide',
        'Domain': 'Conformation',
        'Motif': 'Conformation',
        'PROTEIN_FUNCTION': 'PROTEINS',
        'Transcription_Factor': 'PROTEIN_FUNCTION',
        'Enzyme': 'PROTEIN_FUNCTION',
        'Regulator': 'PROTEIN_FUNCTION',
        'Sigma_Factor': 'Transcription_Factor',
        'Repressor': 'Transcription_Factor',
        'Rho_Factor': 'Transcription_Factor',
        'Activator': 'Transcription_Factor',
        'RNA_Polymerase': 'Enzyme',
        'DNA_Polymerase': 'Enzyme',
        'Restriction_Enzyme': 'Enzyme',

        # NUCLEIC_ACIDS Sub-ontology
        'SEQUENCE&STRUCTURE': 'NUCLEIC_ACIDS',
        'Chromosome': 'SEQUENCE&STRUCTURE',
        'Plasmid': 'SEQUENCE&STRUCTURE',
        'Viral_Vector': 'SEQUENCE&STRUCTURE',
        'RNA': 'SEQUENCE&STRUCTURE',
        'DNA': 'SEQUENCE&STRUCTURE',
        'Locus': 'Chromosome',
        'Gene': 'Chromosome',
        'Operon': 'Chromosome',
        'Mutant_Gene': 'Gene',
        'ORF': 'Gene',
        'Allele': 'Gene',
        'REGULATION&EXPRESSION': 'NUCLEIC_ACIDS',
        'Transcription_Binding_Site': 'REGULATION&EXPRESSION',
        'Termination_Site': 'REGULATION&EXPRESSION',
        'Phenotype': 'REGULATION&EXPRESSION',
        'Ribosome': 'REGULATION&EXPRESSION',
        'mRNA': 'REGULATION&EXPRESSION',
        'Promoter': 'Transcription_Binding_Site',
        'Operator': 'Transcription_Binding_Site',
        'Enhancer': 'Transcription_Binding_Site',

        # LIVING_SYSTEM Sub-ontology
        'Prokaryotes': 'LIVING_SYSTEM',
        'Virus': 'LIVING_SYSTEM',
        'Eukaryotes': 'LIVING_SYSTEM',
        'Bacteria': 'Prokaryotes',
        'Non-Bacteria': 'Prokaryotes',
        #XXX: This one is Wild-Type_Bacteria
        'Wild_Type_Bacteria': 'Bacteria',
        'Mutant_Bacteria': 'Bacteria',
        'Tissue': 'Eukaryotes',
        'Cells': 'Eukaryotes',
        'Organelles': 'Eukaryotes',
        'Organism': 'Eukaryotes',
        'Wild_Type_Organism': 'Organism',
        'Mutant_Organism': 'Organism',

        # PROCESSES Sub-ontology
        'Gene_Expression': 'PROCESSES',
        'Recombination': 'PROCESSES',
        'Mutation': 'PROCESSES',
        'Transcription': 'PROCESSES',
        'Replication': 'PROCESSES',
        'Methylation': 'PROCESSES',
        'Regulation': 'PROCESSES',
        'Gene_Activation': 'Gene_Expression',
        'Gene_Repression': 'Gene_Expression',
        'Insertion': 'Mutation',
        'Deletion': 'Mutation',
        'Null_Mutation': 'Mutation',
        'Point_Mutation': 'Mutation',

        # EXPERIMENTAL Sub-ontology
        'Reagents': 'EXPERIMENTAL',
        'Experimental_Technique': 'EXPERIMENTAL',
        'Experimental_Equipment': 'EXPERIMENTAL',
        'Inorganic_Compounds': 'Reagents',
        'Organic_Compounds': 'Reagents',
        'Biological_Fluids': 'Reagents',
        'Other_Compounds': 'Reagents',
        'Laboratory_Technique': 'Experimental_Technique',
        'Computational_Analysis': 'Experimental_Technique',
        }

# XXX: This will of course crash if you give it an unknown term
def _collapse_grec_term(term):
    # XXX: Check if it is an event trigger
    if term == 'GRE':
        return term

    while True:
        parent = GREC_ONTOLOGY[term]
        if parent in GREC_SUPER_CONCEPTS:
            return parent
        else:
            term = parent

# XXX: Strictly generic terms annotated in GREC
GREC_GENERIC = set(('SPAN', ))
def _filter_generic_grec_terms(doc_it):
    for doc in doc_it:
        for sent in doc:
            sent.annotations = [ann for ann in sent
                    if ann.type not in GREC_GENERIC]
        yield doc

def _alter_terms(doc_it, alt_f):
    for doc in doc_it:
        for sent in doc:
            for ann in sent:
                ann.type = alt_f(ann.type)
        yield doc
###

# A bit ugly but works
def get_grec_set():
    random_seed(0x763f059c)
    paths = [d.path for d in _get_documents(GREC_DIR)]
    train_paths = set(sample(paths, len(paths) / 2))
    paths = [p for p in paths if p not in train_paths]
    dev_paths = set(sample(paths, len(paths) / 2))
    test_paths = set([p for p in paths if p not in dev_paths])
    del paths

    #print 'train', train_paths
    #print 'dev', dev_paths
    #print 'test', test_paths

    return [_filter_generic_grec_terms(it) for it in (
            (d for d in _get_documents(GREC_DIR)
                if d.path in train_paths),
            (d for d in _get_documents(GREC_DIR)
                if d.path in dev_paths),
            (d for d in _get_documents(GREC_DIR)
                if d.path in test_paths),
            )]

def get_super_grec_set():
    return [_alter_terms(it, _collapse_grec_term) for it in get_grec_set()]
    
# A bit ugly but works
def get_nlpba_set():
    random_seed(0x25e453f9)
    paths = [d.path for d in _get_documents(NLPBA_DIR)]
    train_paths = set(sample(paths, len(paths) / 2))
    paths = [p for p in paths if p not in train_paths]
    dev_paths = set(sample(paths, len(paths) / 2))
    test_paths = set([p for p in paths if p not in dev_paths])
    del paths

    #print 'train', train_paths
    #print 'dev', dev_paths
    #print 'test', test_paths

    return (
            (d for d in _get_documents(NLPBA_DIR)
                if d.path in train_paths),
            (d for d in _get_documents(NLPBA_DIR)
                if d.path in dev_paths),
            (d for d in _get_documents(NLPBA_DIR)
                if d.path in test_paths),
            )

def get_nlpba_down_set():
    random_seed(0x8a3403ed)
    
    paths = [d.path for d in _get_documents(NLPBA_DOWN_DIR)]
    train_paths = set(sample(paths, len(paths) / 2))
    paths = [p for p in paths if p not in train_paths]
    dev_paths = set(sample(paths, len(paths) / 2))
    test_paths = set([p for p in paths if p not in dev_paths])
    del paths

    return (
            (d for d in _get_documents(NLPBA_DOWN_DIR)
                if d.path in train_paths),
            (d for d in _get_documents(NLPBA_DOWN_DIR)
                if d.path in dev_paths),
            (d for d in _get_documents(NLPBA_DOWN_DIR)
                if d.path in test_paths),
            )


    

# XXX: We have essentially used Genia to create dictionaries, remove them
SETS_BY_ID = {
        'BioNLP-ST-2011-Epi_and_PTM':           get_epi_set,
        'BioNLP-ST-2011-genia':                 get_genia_set,
        'BioNLP-ST-2011-Infectious_Diseases':   get_id_set,
        ##'BioNLP-ST-2009':                       get_bionlp_2009_set,
        'GREC':                                 get_grec_set,
        'SUPER_GREC':                           get_super_grec_set,
        'CALBC_II':                             get_calbc_ii_set,
        'NLPBA':                                get_nlpba_set,
        'NLPBA_DOWN':                           get_nlpba_down_set,
        }

def _document_stats(docs):
    doc_cnt = 0
    snt_cnt = 0
    ann_cnt = 0
    from collections import defaultdict
    ann_cnt_by_type = defaultdict(int)
    for doc in docs:
        doc_cnt += 1
        for snt in doc:
            snt_cnt += 1
            for ann in snt:
                ann_cnt += 1
                ann_cnt_by_type[ann.type] += 1

    print 'Documents:', doc_cnt
    print 'Sentences:', snt_cnt
    ann_sum = sum(ann_cnt_by_type.itervalues())
    print 'Annotations:', ann_sum
    if ann_sum:
        print 'Annotations by type:'
        for type, cnt in ((t, ann_cnt_by_type[t])
                for t in sorted([k for k in ann_cnt_by_type])):
            print '\t' + type + ': ' + _str(cnt), '({0:.2%})'.format(
                    cnt / float(ann_cnt))

PLOT_AND_TABLES = True
FULL_SET = False
#PLOT_CUT_OFF = 10

def grey_scale(steps):
    limit = 40
    ret = [str((100 - shade) / 100.0)
            for shade in range(0, 100 - limit + 1, (100 - limit) / 5)]
    return ret

def _plot_and_tables(name, docs):
    try:
        import matplotlib
        import matplotlib.pyplot as plt
        PLOT = True
    except ImportError:
        PLOT = False

    from collections import defaultdict

    ann_cnt = 0
    ann_cnt_by_type = defaultdict(int)
    
    for doc in docs:
        for snt in doc:
            for ann in snt:
                ann_cnt += 1
                ann_cnt_by_type[ann.type] += 1

    types = [k for k in ann_cnt_by_type]
    types.sort()
    fracs = [ann_cnt_by_type[t] / float(ann_cnt) for t in types]
    occs = [ann_cnt_by_type[t] for t in types]
    labels = [t.replace('_', ' ') for t in types]

    if PLOT:
        #if len(ann_cnt_by_type) <= PLOT_CUT_OFF:
        c_scale = grey_scale(len(labels))
        plt.pie(fracs, labels=labels,
                autopct='%1.1f%%', shadow=False, colors=c_scale)
        #plt.title('Annotated Word Types')

        for fmt in ('svg', 'png'):
            plt.savefig((id + '_pie' + '.' + fmt).lower(), format=fmt)
        plt.cla()
        plt.clf()
    #else:
    # TODO: Generate which order it is, largest, etc.
    with open((id + '_table.tex').lower(), 'w') as table_file:
        table_file.write('\n'.join((
            '\\begin{tabular}{lrr}',
            '\\toprule',
            'Type & Ratio & Annotations \\\\',
            '\\midrule',
            )))
        table_file.write('\n')
        for l, r, o in zip(labels, fracs, occs):
            table_file.write('{0} & {1:.1f}\\% & {2} \\\\\n'.format(l, r * 100, o))
        table_file.write('\n'.join((
            '\\midrule',
            'Total: & & {0} \\\\'.format(sum(ann_cnt_by_type.itervalues())),
            '\\bottomrule',
            '\\end{tabular}',
            )))

PRINT_STATS = False

if __name__ == '__main__':

    #print 'GREC'
    #_document_stats(chain(*(SETS_BY_ID['GREC']())))
    #print
    #print 'SUPER_GREC'
    #_document_stats(chain(*(SETS_BY_ID['SUPER_GREC']())))
    #exit(0)
    # TODO: Check if dirs exist and generates files!
    # We just iterate over all of it to detect errors
    for i, id, get_set in ((j, t[0], t[1])
            for j, t in enumerate(SETS_BY_ID.iteritems())):
        if i != 0:
            print
        print id

        if PRINT_STATS:
            train, dev, test = get_set()

            print 'Train Set'
            _document_stats(train)
            print
            print 'Development Set'
            _document_stats(dev)
            print
            print 'Test Set'
            _document_stats(test)

        if FULL_SET:
            print 'Full Set'
            _document_stats(chain(*get_set()))


        if PLOT_AND_TABLES:
            print 'Plotting...',
            _plot_and_tables(id, chain(*get_set()))
            print 'Done!'
